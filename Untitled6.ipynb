{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "nZ1Z_hV6NMfC",
        "outputId": "37a268f0-64bb-4121-edf6-f9875bf1c69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Train_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-811265e07043>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#and various other tools for data manipulation, visualization, and machine learning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train_data.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "import warnings\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "import itertools\n",
        "from xgboost import XGBClassifier\n",
        "from tabulate import tabulate\n",
        "from google.colab import drive\n",
        "import time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#These lines import necessary libraries and modules, such as NumPy, Pandas, Seaborn, Matplotlib, scikit-learn classifiers,\n",
        "#and various other tools for data manipulation, visualization, and machine learning.\n",
        "\n",
        "train = pd.read_csv('Train_data.csv')\n",
        "test = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Display basic information about the training data (Exploratary Data Analysis)\n",
        "print(train.head())\n",
        "print(train.info())\n",
        "print(train.describe())\n",
        "print(train.describe(include='object'))\n",
        "print(train.shape)\n",
        "print(train.isnull().sum())\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(f\"Number of duplicate rows: {train.duplicated().sum()}\")\n",
        "\n",
        "# Visualize class distribution\n",
        "sns.countplot(x=train['class'])\n",
        "print('Class distribution Training set:')\n",
        "print(train['class'].value_counts())\n",
        "\n",
        "# Label encoding for categorical columns\n",
        "def le(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            label_encoder = LabelEncoder()\n",
        "            df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "le(train)\n",
        "le(test)\n",
        "\n",
        "# Drop unnecessary column\n",
        "train.drop(['num_outbound_cmds'], axis=1, inplace=True)\n",
        "test.drop(['num_outbound_cmds'], axis=1, inplace=True)\n",
        "\n",
        "# Feature selection using Recursive Feature Elimination (RFE)\n",
        "X_train = train.drop(['class'], axis=1)\n",
        "Y_train = train['class']\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "rfe = RFE(rfc, n_features_to_select=10)\n",
        "rfe = rfe.fit(X_train, Y_train)\n",
        "feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), X_train.columns)]\n",
        "selected_features = [v for i, v in feature_map if i==True]\n",
        "X_train = X_train[selected_features]\n",
        "\n",
        "# Standard scaling\n",
        "scale = StandardScaler()\n",
        "X_train = scale.fit_transform(X_train)\n",
        "test = scale.fit_transform(test)\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, train_size=0.70, random_state=2)\n",
        "\n",
        "# Logistic Regression model\n",
        "clfl = LogisticRegression(max_iter=1200000)\n",
        "start_time = time.time()\n",
        "clfl.fit(x_train, y_train.values.ravel())\n",
        "end_time = time.time()\n",
        "print(\"Training time: \", end_time-start_time)\n",
        "start_time = time.time()\n",
        "y_test_pred = clfl.predict(x_train)\n",
        "end_time = time.time()\n",
        "print(\"Testing time: \", end_time-start_time)\n",
        "\n",
        "lg_model = LogisticRegression(random_state=42)\n",
        "lg_model.fit(x_train, y_train)\n",
        "lg_train, lg_test = lg_model.score(x_train, y_train), lg_model.score(x_test, y_test)\n",
        "\n",
        "print(f\"Training Score: {lg_train}\")\n",
        "print(f\"Test Score: {lg_test}\")\n",
        "\n",
        "# Install Optuna if not already installed\n",
        "!pip install optuna\n",
        "import optuna\n",
        "#This defines an objective function for the hyperparameter optimization of the K-Nearest Neighbors (KNN) model using Optuna.\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Define objective function for KNN model optimization\n",
        "def objective(trial):\n",
        "    n_neighbors = trial.suggest_int('KNN_n_neighbors', 2, 16, log=False)\n",
        "    classifier_obj = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    classifier_obj.fit(x_train, y_train)\n",
        "    accuracy = classifier_obj.score(x_test, y_test)\n",
        "    return accuracy\n",
        "\n",
        "# Optimize KNN model\n",
        "study_KNN = optuna.create_study(direction='maximize')\n",
        "study_KNN.optimize(objective, n_trials=1)\n",
        "print(study_KNN.best_trial)\n",
        "\n",
        "KNN_model = KNeighborsClassifier(n_neighbors=study_KNN.best_trial.params['KNN_n_neighbors'])\n",
        "KNN_model.fit(x_train, y_train)\n",
        "\n",
        "KNN_train, KNN_test = KNN_model.score(x_train, y_train), KNN_model.score(x_test, y_test)\n",
        "\n",
        "print(f\"Train Score: {KNN_train}\")\n",
        "print(f\"Test Score: {KNN_test}\")\n",
        "\n",
        "# Decision Tree model\n",
        "clfd = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
        "start_time = time.time()\n",
        "clfd.fit(x_train, y_train.values.ravel())\n",
        "end_time = time.time()\n",
        "print(\"Training time: \", end_time-start_time)\n",
        "start_time = time.time()\n",
        "y_test_pred = clfd.predict(x_train)\n",
        "end_time = time.time()\n",
        "print(\"Testing time: \", end_time-start_time)\n",
        "\n",
        "# Define objective function for Decision Tree model optimization\n",
        "def objective(trial):\n",
        "    dt_max_depth = trial.suggest_int('dt_max_depth', 2, 32, log=False)\n",
        "    dt_max_features = trial.suggest_int('dt_max_features', 2, 10, log=False)\n",
        "    classifier_obj = DecisionTreeClassifier(max_features=dt_max_features, max_depth=dt_max_depth)\n",
        "    classifier_obj.fit(x_train, y_train)\n",
        "    accuracy = classifier_obj.score(x_test, y_test)\n",
        "    return accuracy\n",
        "\n",
        "# Optimize Decision Tree model\n",
        "study_dt = optuna.create_study(direction='maximize')\n",
        "study_dt.optimize(objective, n_trials=30)\n",
        "print(study_dt.best_trial)\n",
        "\n",
        "dt = DecisionTreeClassifier(max_features=study_dt.best_trial.params['dt_max_features'],\n",
        "                            max_depth=study_dt.best_trial.params['dt_max_depth'])\n",
        "dt.fit(x_train, y_train)\n",
        "\n",
        "dt_train, dt_test = dt.score(x_train, y_train), dt.score(x_test, y_test)\n",
        "\n",
        "print(f\"Train Score: {dt_train}\")\n",
        "print(f\"Test Score: {dt_test}\")\n",
        "\n",
        "# Display results in a tabular format\n",
        "data = [[\"KNN\", KNN_train, KNN_test],\n",
        "        [\"Logistic Regression\", lg_train, lg_test],\n",
        "        [\"Decision Tree\", dt_train, dt_test]]\n",
        "\n",
        "col_names = [\"Model\", \"Train Score\", \"Test Score\"]\n",
        "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "# Model Validation using cross-validation\n",
        "SEED = 42\n",
        "\n",
        "# Decision Tree Model\n",
        "dtc = DecisionTreeClassifier(random_state=SEED)\n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Logistic Regression MODEL\n",
        "lr = LogisticRegression()\n",
        "\n",
        "models = {'KNeighborsClassifier': knn, 'LogisticRegression': lr, 'DecisionTreeClassifier': dtc}\n",
        "\n",
        "scores = {}\n",
        "for name in models:\n",
        "    scores[name] = {}\n",
        "    for scorer in ['precision', 'recall']:\n",
        "        scores[name][scorer] = cross_val_score(models[name], x_train, y_train, cv=10, scoring=scorer)\n",
        "\n",
        "def line(name):\n",
        "    return '*' * (25 - len(name) // 2)\n",
        "\n",
        "for name in models:\n",
        "    print(line(name), name, 'Model Validation', line(name))\n",
        "\n",
        "    for scorer in ['precision', 'recall']:\n",
        "        mean = round(np.mean(scores[name][scorer]) * 100, 2)\n",
        "        stdev = round(np.std(scores[name][scorer]) * 100, 2)\n",
        "        print(\"Mean {}: {}\\n+-{}\".format(scorer, mean, stdev))\n",
        "        print()\n",
        "\n",
        "# Model Testing and Evaluation\n",
        "models = {'KNeighborsClassifier': knn, 'LogisticRegression': lr, 'DecisionTreeClassifier': dtc}\n",
        "\n",
        "preds = {}\n",
        "for name in models:\n",
        "    models[name].fit(x_train, y_train)\n",
        "    preds[name] = models[name].predict(x_test)\n",
        "print(\"Predictions complete.\")\n",
        "\n",
        "# Display confusion matrix, classification report, and F1 score for each model\n",
        "def line(name, sym=\"*\"):\n",
        "    return sym * (25 - len(name) // 2)\n",
        "\n",
        "target_names = [\"normal\", \"anomaly\"]\n",
        "for name in models:\n",
        "    print(line(name), name, 'Model Testing', line(name))\n",
        "    print(confusion_matrix(y_test, preds[name]))\n",
        "    print(line(name, '-'))\n",
        "    print(classification_report(y_test, preds[name], target_names=target_names))\n",
        "    f1s = {}\n",
        "\n",
        "# Display F1 scores in a bar plot\n",
        "for name in models:\n",
        "    f1s[name] = f1_score(y_test, preds[name])\n",
        "\n",
        "f1s = pd.DataFrame(f1s.values(), index=f1s.keys(), columns=[\"F1-score\"]) * 100\n",
        "f1s.plot(kind=\"bar\", ylim=[80, 100], figsize=(10, 6), rot=0)"
      ]
    }
  ]
}